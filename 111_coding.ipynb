{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/twisha-k/Python_notes/blob/main/111_coding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "q6MczvPSAn88"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_RIOW8yUrd0"
      },
      "source": [
        "#Lesson 111: K-Means Clustering - Outliers Removal and Customer Segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIDGouSqn85Y"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EL8AB1d5_zeX"
      },
      "source": [
        "### Teacher-Student Activities\n",
        "\n",
        "In the previous class, we learned RFM analysis for analysing customers based on three factors: Recency, Frequency, and Monetary Value.\n",
        "\n",
        "In this class, we will remove outliers from the dataset and apply K-Means clustering to create clusters of customers exhibiting similar purchase behaviour.\n",
        "\n",
        "Let's quickly run the code cells and go through the problem statement covered in the previous lesson and begin this lesson from **Activity 1: Data Analysis**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPgOQQlKEUm7"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ij_TzinVbGn8"
      },
      "source": [
        "#### Customer Segmentation Problem Statement\n",
        "\n",
        "\n",
        "We have a transactional dataset that contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail. The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.\n",
        "\n",
        "The company wants to segment its customers and determine marketing strategies according to these segments.\n",
        "\n",
        "The dataset consists of the following attributes:\n",
        "\n",
        "- `InvoiceNo`: Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation.\n",
        "\n",
        "- `StockCode`: Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product.\n",
        "\n",
        "- `Description`: Product (item) name. Nominal.\n",
        "\n",
        "- `Quantity`: The quantities of each product (item) per transaction. Numeric.\n",
        "\n",
        "- `InvoiceDate`: Invoice date and time. Numeric, the day and time when each transaction was generated. The date-time format used here is `yyyy-mm-dd hh:mm:ss`.\n",
        "\n",
        "- `UnitPrice`: Unit price. Numeric, product price per unit in pound sterling, also known as GBP (Great Britain Pound).\n",
        "\n",
        "- `CustomerID`: Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer.\n",
        "\n",
        "- `Country`: Country name. Nominal, the name of the country where each customer resides.\n",
        "\n",
        "\n",
        "\n",
        "**Dataset Credits:** https://archive.ics.uci.edu/ml/datasets/online+retail\n",
        "\n",
        "**Citation:** Dr Daqing Chen, Director: Public Analytics group. chend '@' lsbu.ac.uk, School of Engineering, London South Bank University, London SE1 0AA, UK.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ykAJfZMbxmD"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Recap"
      ],
      "metadata": {
        "id": "_4GTwikPDLoW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxtni1ljbyeS"
      },
      "source": [
        "#### Loading the Dataset\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Dataset Link:** https://s3-student-datasets-bucket.whjr.online/whitehat-ds-datasets/online-retail-customers.xlsx\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRmuQdXdbw1j"
      },
      "source": [
        "# Read the dataset and create a Pandas DataFrame.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "file_path = \"https://s3-student-datasets-bucket.whjr.online/whitehat-ds-datasets/online-retail-customers.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Eqlk8JZcl0U"
      },
      "source": [
        "# Get the total number of rows and columns, data types of columns and missing values (if exist) in the dataset.\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLLG2Wepu3hX"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrTg8DbrYas4"
      },
      "source": [
        "#### Removing the Cancelled Orders\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQV34_y6Zt-O"
      },
      "source": [
        "# Check the data type of 'InvoiceNo' field\n",
        "type(df['InvoiceNo'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQBlGdpZdiyu"
      },
      "source": [
        "# Convert 'InvoiceNo' field to string and verify whether the data type is converted or not.\n",
        "df['InvoiceNo'] = df['InvoiceNo'].astype('str')\n",
        "type(df['InvoiceNo'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4SkZY-ZcodY"
      },
      "source": [
        "# Use regex to find 'C' in the 'InvoiceNo' field\n",
        "import re\n",
        "df[df['InvoiceNo'].str.contains(pat = 'C', flags = re.IGNORECASE)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngZZCcVYke_G"
      },
      "source": [
        "# Check total number of orders including cancelled orders.\n",
        "df['InvoiceNo'].shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6rFuUpLn0jm"
      },
      "source": [
        "# Remove canceleled invoices from the dataset\n",
        "df = df[~(df['InvoiceNo'].str.contains('C', flags = re.IGNORECASE, regex = True))]\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohMvY9Fiu14H"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-EtcNJLugom"
      },
      "source": [
        "#### Removing Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_FqhoLopf2f"
      },
      "source": [
        "# Obtain the number of missing or null values in df\n",
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RMFyvJKqxph"
      },
      "source": [
        "# Determine the percentage of null values in each column.\n",
        "df.isnull().sum() * 100 / df.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wyrk8xrgtDOf"
      },
      "source": [
        "# Remove the null valued rows.\n",
        "print(f\"Before removing null values:\\nNumber of rows = {df.shape[0]}\")\n",
        "df.dropna(inplace = True)\n",
        "print(f\"After removing null values:\\nNumber of rows = {df.shape[0]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmO1TgxPtxgG"
      },
      "source": [
        "# Again obtain the number of null values in df.\n",
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvR3hV4duS1m"
      },
      "source": [
        "# Check the data type of CustomerID column.\n",
        "df['CustomerID'].dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUgC1hwh6NeH"
      },
      "source": [
        "# Convert 'CustomerID' field to integer based categorical column.\n",
        "df['CustomerID'] = df['CustomerID'].astype('int64').astype('category')\n",
        "df['CustomerID'].dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGTdeUWCETjj"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5adWsY8fwq24"
      },
      "source": [
        "#### RFM Analysis\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtf2xnHI4kSU"
      },
      "source": [
        "# Check the first 5 samples of the DataFrame\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfitm3N3568v"
      },
      "source": [
        "# Obtain the the total purchase amount for the customers\n",
        "df['TotalPrice'] = df['Quantity'] * df['UnitPrice']\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emt8eHsC6HdN"
      },
      "source": [
        "# Obtain the number of unique customers\n",
        "df['CustomerID'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7bjKMJh7Gj8"
      },
      "source": [
        "# Obtain the Monetary information from the DataFrame\n",
        "monetary_df = df[['CustomerID', 'TotalPrice']].groupby('CustomerID', as_index = False).sum()\n",
        "monetary_df.rename(columns = {'TotalPrice' : 'Monetary'}, inplace = True)\n",
        "monetary_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3Pn8lflBlYc"
      },
      "source": [
        "# Obtain the Frequency information from the DataFrame\n",
        "frequency_df =  df[['CustomerID', 'InvoiceNo']].groupby('CustomerID', as_index = False).count()\n",
        "frequency_df.rename(columns = {'InvoiceNo': 'Frequency'}, inplace = True)\n",
        "frequency_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7ibRz7vjkjS"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6GK3ZSqjliq"
      },
      "source": [
        "#### Merging DataFrames\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lf7DNhq_FTcn"
      },
      "source": [
        "# Merge 'monetary_df' and 'frequency_df' DataFrames.\n",
        "rfm_df = pd.merge(monetary_df, frequency_df, on='CustomerID', how = 'inner')\n",
        "rfm_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSAgWs6Mj4oZ"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvXtAQzCj5ox"
      },
      "source": [
        "#### Calculating Recency\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ol7FvzyXJYMp"
      },
      "source": [
        "# Obtain the last purchase date for each customer\n",
        "recency_df = df[['CustomerID', 'InvoiceDate']].groupby('CustomerID', as_index = False).max()\n",
        "recency_df.rename(columns = {'InvoiceDate': 'LastPurchaseDate'}, inplace = True)\n",
        "recency_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jANm7QEOJHMR"
      },
      "source": [
        "# Obtain the last invoice date in the dataset.\n",
        "df['InvoiceDate'].max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyM7Q6t-pkt6"
      },
      "source": [
        "# Obtain the present date i.e LastPurchaseDate + 1 day\n",
        "present_date = df['InvoiceDate'].max() + pd.Timedelta(\"1 day\")\n",
        "present_date"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzDouss3JnEb"
      },
      "source": [
        "# Obtain the days since last purchase made by a customer\n",
        "days_last_purchase = present_date - recency_df['LastPurchaseDate']\n",
        "days_last_purchase"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBmImRZwLBM9"
      },
      "source": [
        "# Extract days from datetime using 'dt.days' attribute\n",
        "recency_days = days_last_purchase.dt.days\n",
        "recency_days"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Uz08HuRN3k9"
      },
      "source": [
        "# Add 'recency_days' as column to the merged DataFrame 'rfm_df'.\n",
        "rfm_df['Recency'] = recency_days\n",
        "rfm_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEuw_RpRNmk0"
      },
      "source": [
        "We now have a DataFrame for RFM analysis consisting of the necessary fields to carry out the customer segmentation.\n",
        "\n",
        "Let us now analyse the `rfm_df` DataFrame obtained after RFM analysis and prepare it for K-Means clustering.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr76aCXF57xb"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlUDfdkh591C"
      },
      "source": [
        "#### Activity 1: Data Analysis\n",
        "\n",
        "For clustering, the `CustomerID` field is not required hence it can be dropped from the `rfm_df` DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCUApKUk9FDS"
      },
      "source": [
        "# S1.1: Dropping the 'CustomerID' column\n",
        "rfm_df.drop(columns = 'CustomerID', inplace = True)\n",
        "rfm_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xHsVQG3BIaj"
      },
      "source": [
        "Let's create histogram and boxplots to understand the distribution of `Monetary`, `Frequency`, and `Recency` columns.\n",
        "\n",
        "Use `subplots()` function of `matplotlib.pyplot` module to display all the three histograms in the first row and the boxplots in the second row.\n",
        "\n",
        "Follow the steps given below to create this subplot:\n",
        "1. Call the `subplots()` function on an object of `matplotlib.pyplot` and unpack the figure and axis objects in two different variables, say `fig` and `axis`. Inside the `subplots()` function, pass:\n",
        "\n",
        "  - `nrows = 2` and `ncols = 3` parameters to create a figure having 2 rows and 3 columns.\n",
        "\n",
        "  - `figsize = (15, 5)` parameter to create the figure of 15 units wide and 5 units high.\n",
        "\n",
        "  - `dpi = 100` parameter to further enlarge the figure based on their pixel density.\n",
        "\n",
        "2. Construct a histogram to visualise the distribution of `Monetary` column using first row, first column subplot's axes i.e `axis[0, 0]`.\n",
        "\n",
        "3. Construct a boxplot to visualise the distribution of `Monetary` column using second row, first column subplot's axes i.e `axis[1, 0]`.\n",
        "\n",
        "4. Also call the `set_title()` function using the `axis[0, 0]` object to set the `title` for histogram and boxplot.\n",
        "\n",
        "5. Similarly, construct histograms and boxplots for `Frequency` and `Recency` columns using the respective subplots's axes.\n",
        "\n",
        "6. Call the `show()` function on the `matplotlib.pyplot` object.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwOV30INBgd6"
      },
      "source": [
        "# S1.2: Obtain the histogram and boxplots\n",
        "fig, axis = plt.subplots(nrows = 2, ncols = 3, figsize = (15, 5), dpi = 100)\n",
        "\n",
        "# Construct Histogram and Boxplot for 'Monetary'\n",
        "axis[0, 0].hist(rfm_df['Monetary'], bins = 'sturges', facecolor = 'red', edgecolor = 'black')\n",
        "sns.boxplot(x = 'Monetary', data = rfm_df, ax = axis[1, 0], color = 'red')\n",
        "axis[0, 0].set_title(\"Histogram and Boxplot for Monetary\")\n",
        "\n",
        "# Construct Histogram and Boxplot for 'Frequency'\n",
        "axis[0, 1].hist(rfm_df['Frequency'], bins = 'sturges', facecolor = 'green', edgecolor = 'black')\n",
        "sns.boxplot(x = 'Frequency', data = rfm_df, ax = axis[1, 1], color = 'green')\n",
        "axis[0, 1].set_title(\"Histogram and Boxplot for Frequency\")\n",
        "\n",
        "# Construct Histogram and Boxplot for 'Recency'\n",
        "axis[0, 2].hist(rfm_df['Recency'], bins = 'sturges', facecolor = 'purple', edgecolor = 'black')\n",
        "sns.boxplot(x = 'Recency', data = rfm_df, ax = axis[1, 2], color = 'purple')\n",
        "axis[0, 2].set_title(\"Histogram and Boxplot for Recency\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erXm81uSG006"
      },
      "source": [
        "From the above plot it is clear there are lot of outliers in `Monetary` and `Frequency` fields.\n",
        "\n",
        "These outliers will affect the model as the K-Means clustering is based on the distance of data points from the cluster centroids. These outliers will shift the cluster centroids away from their intended positions thereby generating inaccurate clusters. To compensate for this we need to remove the outliers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47PeqVCaHi6j"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tok3MlQcvFkv"
      },
      "source": [
        "#### Activity 2: Removing Outliers\n",
        "\n",
        "We had already learned how boxplots are useful in identifying outliers in column data in one of the previous lessons (*Lesson: Meteorite Landings - Box Plots*). Let us recall that.\n",
        "\n",
        "**What are outliers?**\n",
        "- Outlier is a value in a data series that is either very small or very large.\n",
        "- Outliers are abnormal values that can affect the overall observation due to its very high or very low extreme values.\n",
        "- Hence they should be removed from the actual data.\n",
        "\n",
        "The best way to detect outliers is to create a boxplot. It plots the minimum, first quartile, second quartile, third quartile, and maximum values in the form of a box. Any value beyond minimum and maximum limit is considered as an outlier.\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src= \"https://s3-whjr-v2-prod-bucket.whjr.online/fc916def-1fd4-4a16-8a7f-caadecafdecc.jpg\" height = 350 /></center>\n",
        "\n",
        "\n",
        "- **Median or Second quartile ($Q2$):** The middle value of the dataset. Also known as $50^\\text{th}$ percentile.\n",
        "\n",
        "- **First quartile ($Q1$):** The middle value between the smallest value (not the \"minimum\") and the median of the dataset. Also known as $25^\\text{th}$ percentile which means that $25\\%$ of the data lies between smallest value and $Q1$.\n",
        "\n",
        "- **Third quartile ($Q3$):** The middle value between the median and the highest value (not the \"maximum\") of the dataset. Also known as  $75^\\text{th}$ percentile which means 75% of the data lies between smallest value and $Q3$.\n",
        "\n",
        "- **InterQuartile Range ($IQR$):**  $25^\\text{th}$ to the  $75^\\text{th}$ percentile. $IQR$ tells how spread the middle values are. It is defined as:\n",
        "\n",
        "\\begin{align}\n",
        "IQR = Q3 - Q1\n",
        "\\end{align}\n",
        "\n",
        "- **Minimum or Lower Bound:** $Q1 -1.5 \\times IQR$\n",
        "\n",
        "- **Maximum or Upper Bound:** $Q3 + 1.5 \\times IQR$\n",
        "\n",
        "- **Outliers:** These are the points that lies beyond the \"Minimum\" and \"Maximum\" value. So any value more than the upper bound or lesser than the lower bound will be considered as outliers.\n",
        "\n",
        "Let's define a `remove_outliers()` function which removes outlier from the column data and returns an outlier free DataFrame. This function takes two parameters as input:\n",
        " - `df`: The DataFrame which consists of columns containing outliers.\n",
        " - `col`: The column of DataFrame `df` from which the outliers needs to be flushed out.\n",
        "\n",
        "Inside this function,\n",
        "\n",
        "1. Calculate $Q1$ or $25^\\text{th}$ quartile for `col` column using `quantile()` function of `pandas` module and store it in a `q1` variable. Pass `0.25` as input to `quantile()` function.\n",
        "\n",
        "  **Syntax of `quantile()` function:** `DataFrame.quantile(q)` where, `q` is the quantile to be computed. By default, `q = 0.5` ($50\\%$ quantile)\n",
        "\n",
        "2. Calculate $Q3$ or $75^\\text{th}$ quartile for `col` column using `quantile()` function and store it in a `q3` variable. Pass `0.75` as input to `quantile()` function.\n",
        "\n",
        "3. Calculate $IQR$ by subtracting `q3` from `q1` and store it in a `iqr` variable.\n",
        "\n",
        "4. Calculate lower bound and upper bound using the following formula and store it in `lower_bound` and `upper_bound` variables respectively.\n",
        "\n",
        "$$\\text{Lower Bound}=Q1 - 1.5 \\times IQR$$\n",
        "$$\\text{Upper Bound}=Q3 + 1.5 \\times IQR$$\n",
        "\n",
        "5. Obtain only those values from `df` DataFrame which matches the following condition:\n",
        "\n",
        "    `(df[col] >= lower_bound) & (df[col] <= upper_bound)`\n",
        "\n",
        "  This condition will return those values of the `col` column which are between lower bound and upper bound.\n",
        "\n",
        "6. Return the filtered DataFrame.\n",
        "\n",
        "**Note:** Here, the terms **quartile** and **quantile** are being used interchangeably. However, quantile is something that divides the dataset into equal parts. A quantile that divides the dataset into 4 equal parts i.e. at 0.25, 0.5 , 0.75, 1.00 is called a quartile. Thus, quartile is a type of quantile.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlQr20q0ZcTG"
      },
      "source": [
        "# S2.1: Create a function for removing the outliers.\n",
        "def remove_outliers(df, col):\n",
        "  q1 = df[col].quantile(0.25)  # Q1\n",
        "  q3 = df[col].quantile(0.75)  # Q3\n",
        "  iqr = q3 - q1                # IQR = Q3 - Q1\n",
        "  lower_bound =  q1 - 1.5 * iqr  # lower_bound = Q1 − 1.5 * IQR\n",
        "  upper_bound = q3 + 1.5 * iqr  # upper_bound = Q3 + 1.5 * IQR\n",
        "  new_df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
        "\n",
        "  return new_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQ6LVw5zX_qN"
      },
      "source": [
        "Now that we have created a function for removing outliers, we can easily remove outliers from `rfm_df` DataFrame.\n",
        "\n",
        "To remove the outliers from the `Monetary` field:\n",
        "\n",
        "1. Call the `remove_outliers()` function and pass `rfm_df` and  `'Monetary'` as input to this function. Save the returned DataFrame in a `m_clean_df` variable.\n",
        "\n",
        "2. Reset the index of `m_clean_df` DataFrame using `reset_index(drop = True)` function. This function deletes the old index and resets the index in the new DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Eo4mCZoanQf"
      },
      "source": [
        "# S2.2: Removing outliers from 'Monetary' field\n",
        "m_clean_df = remove_outliers(rfm_df, 'Monetary')\n",
        "m_clean_df = m_clean_df.reset_index(drop = True)\n",
        "m_clean_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKLE8DjjZ4W4"
      },
      "source": [
        "The `rfm_df` had 4339 rows and after removal of outliers `m_clean_df` has 3912 rows which means there were $4339 - 3912 = 427$ outliers in the `Monetary` field.\n",
        "\n",
        "Let us again create boxplots for the `Recency`, `Monetary`, and `Frequency` field and observe whether there is any improvement in data distribution. Use subplots to create these multiple plots."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gr8ZqiuO-Ivh"
      },
      "source": [
        "# S2.3: Obtain the boxplots\n",
        "fig, axis = plt.subplots(nrows = 1, ncols = 3, figsize = (15, 5), dpi = 100)\n",
        "\n",
        "# Construct Boxplot for 'Monetary'\n",
        "sns.boxplot(x = 'Monetary', data = m_clean_df, ax = axis[0], color = 'red')\n",
        "axis[0].set_title(\"Boxplot for Monetary\")\n",
        "\n",
        "# Construct Boxplot for 'Frequency'\n",
        "sns.boxplot(x = 'Frequency', data = m_clean_df, ax = axis[1], color = 'green')\n",
        "axis[1].set_title(\"Boxplot for Frequency\")\n",
        "\n",
        "# Construct Boxplot for 'Recency'\n",
        "sns.boxplot(x = 'Recency', data = m_clean_df, ax = axis[2], color = 'purple')\n",
        "axis[2].set_title(\"Boxplot for Recency\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2R2xjsAbDjQ"
      },
      "source": [
        "\n",
        "We can observe that lot of outliers has been removed from the `Monetary` and `Frequency` columns.\n",
        "\n",
        "Next, let us standardise the  DataFrame, so that all the columns have mean equals to `0` and the standard deviation equals to `1`. For this,\n",
        "1. Create an object of `StandardScaler()` class of `sklearn.preprocessing` module.\n",
        "2. Apply `fit_transform()` function on `mf_cleaned_df` DataFrame and store the scaled values in a new DataFrame `scaled_df`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7jdL5kcbRVS"
      },
      "source": [
        "# S2.4: Normalise the RFM parameters\n",
        "# Import StandardScaler Module from sklearn\n",
        "\n",
        "# Make an object 'StandardScaler()'\n",
        "\n",
        "\n",
        "# Perform fit and transform operation using 'fit_transform()'\n",
        "\n",
        "\n",
        "# Make a new DataFrame\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuTSHQ32n8vP"
      },
      "source": [
        "Let us again create histograms for `Monetary`, `Frequency`, and `Recency` columns to check whether all of them have similar mean and variance after standardisation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SOmhu7DePra"
      },
      "source": [
        "# S2.5: Obtain the histograms.\n",
        "fig, axis = plt.subplots(nrows = 1, ncols = 3, figsize = (15, 5), dpi = 100)\n",
        "\n",
        "# Construct Histogram for 'Monetary'\n",
        "axis[0].hist(scaled_df['Monetary'], bins = 'sturges', facecolor = 'red', edgecolor = 'black')\n",
        "axis[0].set_title(\"Histogram for Monetary\")\n",
        "\n",
        "# Construct Histogram for 'Frequency'\n",
        "axis[1].hist(scaled_df['Frequency'], bins = 'sturges', facecolor = 'green', edgecolor = 'black')\n",
        "axis[1].set_title(\"Histogram for Frequency\")\n",
        "\n",
        "# Construct Histogram for 'Recency'\n",
        "axis[2].hist(scaled_df['Recency'], bins = 'sturges', facecolor = 'purple', edgecolor = 'black')\n",
        "axis[2].set_title(\"Histogram for Recency\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PabPo2PYqSyz"
      },
      "source": [
        "You may note that all the columns now have same mean and variance. Now our DataFrame is ready for K-Means clustering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DQn03jBrutb"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYlKSTPcrvyq"
      },
      "source": [
        "#### Activity 3: Applying K-Means Clustering\n",
        "\n",
        "We start by finding the optimal number of clusters for the K-Means algorithm. We will use the Elbow method.\n",
        "\n",
        "Recall the steps for Elbow method:\n",
        "1. Compute K-Means clustering for different values of `K` by varying `K` from `1` to `10` clusters.\n",
        "2. For each K, calculate the total within-cluster sum of square (WCSS) using `inertia_` attribute of `KMeans` object.\n",
        "3. Plot the curve of WCSS vs the number of clusters `K`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxgthkF7sO85"
      },
      "source": [
        "# S3.1: Determine 'K' using Elbow method.\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "wcss = []\n",
        "\n",
        "clusters = range(1, 11)\n",
        "# Initiate a for loop that ranges from 1 to 10.\n",
        "for k in clusters:\n",
        "    # Inside for loop, perform K-means clustering for current value of K. Use 'fit()' to train the model.\n",
        "    kmeans = KMeans(n_clusters = k, random_state = 10)\n",
        "    kmeans.fit(scaled_df)\n",
        "    # Find wcss for current K value using 'inertia_' attribute and append it to the empty list.\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "# Plot WCSS vs number of clusters.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PF0Q3Q8EsmHT"
      },
      "source": [
        "\n",
        "\n",
        "From the above plot, it looks like decrease starts to slow down between 3 and 5. So you can choose any number of clusters from 3 to 5. Let us use 4 clusters to perform K-Means clustering.\n",
        "\n",
        "Now, perform K-Means clustering with `n_clusters = 4` parameter and determine the cluster labels. Also, count the number of customers in each cluster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oy6t4ORDtVgo"
      },
      "source": [
        "# S3.2: Clustering the dataset for K = 4\n",
        "\n",
        "\n",
        "# Perform K-Means clustering with n_clusters = 4 and random_state = 10\n",
        "\n",
        "\n",
        "# Fit the model to the scaled_df\n",
        "\n",
        "\n",
        "# Make a series using predictions by K-Means\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ku2qtYWx83h"
      },
      "source": [
        "As you can see here, the data is divided into $4$ clusters labelled from `0` to `3`. For cluster visualisation we will not plot normalised DataFrame for scatter graph as the plot's axis won't convey any meaningful information.\n",
        "\n",
        "Let's make a new DataFrame `km_df` by concatenating `mf_clean_df` and `cluster_labels` using `concat()` function of `pandas` module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfK8FuvIxTkO"
      },
      "source": [
        "# S3.4: Create a DataFrame with cluster labels for cluster visualisation\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wotaLOuW8UpF"
      },
      "source": [
        "The cluster labels for all the data points are now obtained, let's display those clusters using `scatter_3d()` function from `plotly.express` module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWQ-ushIsluX"
      },
      "source": [
        "# S3.5: Visualising the clusters for customer segmentation\n",
        "import plotly.express as px\n",
        "plotly_fig = px.scatter_3d(km_df, x = 'Monetary', y = 'Frequency', z = 'Recency', color = 'Cluster Label')\n",
        "plotly_fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Idc4BFaQ_Zqk"
      },
      "source": [
        "**Summarising clusters:**\n",
        "\n",
        "Let us calculate the mean recency, frequency, and monetary values of all the clusters by applying `agg()` function on `km_df` DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "od8BjnB78v0O"
      },
      "source": [
        "# S3.6: Understanding the Cluster Distribution\n",
        "mean_df = km_df.groupby(['Cluster Label']).agg({'Recency':['mean'],\n",
        "                                              'Frequency':['mean'],\n",
        "                                              'Monetary':['mean','count']}).round(0)\n",
        "mean_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9-IggjKDpV_"
      },
      "source": [
        "The above dataframe gives an optimal interpretation of clusters. Let us understand what each cluster represent.\n",
        "\n",
        "\n",
        "\n",
        "- The <b><font color = blue>first cluster</font></b> (label 3) belongs to the \"Promising Customers\" segment as:\n",
        "    - They purchased recently (`R = 52 days`).\n",
        "    - Average purchase frequency is very less (`F = 39 purchases`).\n",
        "    - They spend little (`M = 573 GBP`).\n",
        "\n",
        "- The <b><font color= purple>second cluster</font></b> (label 0) belongs to the \"Almost Lost Customers\" segment as:\n",
        "  - Their last purchase  is long ago (`R = 255 days`).\n",
        "  - Average purchase frequency is very less (`F = 24 purchases`).\n",
        "  - They spend very little (`M = 380 GBP`).\n",
        "\n",
        "- The \"best customers\" are in <b><font color = orange>third cluster</font></b> (label 2) and <b><mark>fourth cluster</mark></b> (label 1). They spent the greatest amount of money, made many purchases and their last purchase was few days before.\n",
        "\n",
        "Hence, we can see that using K-Means clustering we divided customers into clusters. Customers in each cluster have similar buying behaviours, so we can use them to personalise marketing offers.\n",
        "\n",
        "But there are certain challenges with K-means. Let us discuss them one by one.\n",
        "\n",
        "1. **Clustering outliers:** Cluster centroids can be dragged by outliers, or outliers might get their own cluster instead of being ignored. Hence outliers have to be removed as K-Means clustering is highly sensitive to outliers.\n",
        "\n",
        "2. **`K`  has to be chosen manually:** If number of clusters are unknown you have to use the \"WCSS vs Clusters\" plot to find the optimal value of `K`.\n",
        "\n",
        "3. K-Means algorithm is good in capturing the structure of the data if clusters have a spherical-like shape. If the clusters have  complicated geometric shapes, K-Means does a poor job in clustering the data.\n",
        "\n",
        "To handle above limitations of K-Means we can use **Agglomerative/Hierarchical Clustering** or **PCA - Principle Component Analysis** that we will explore in the upcoming lessons.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVCYeU-G2a6k"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kohzTA6Fq_Jy"
      },
      "source": [
        "### **Project**\n",
        "You can now attempt the **Applied Tech Project 111 - KMeans Clustering V** on your own.\n",
        "\n",
        "**Applied Tech Project 111 - KMeans Clustering V** : https://colab.research.google.com/drive/1BHChmfN2JTvB-Zh_2RfEgo8_npdqD66c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tboXI7zXrSNW"
      },
      "source": [
        "---"
      ]
    }
  ]
}